{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"green\"> | - Questions théoriques :</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relire le cours sur Elastic Search à l’aide de nos nouvelles connaissance et répondre aux questions:\n",
    "1) Qu’est ce que le sharding, comment pourrait-on imaginer un sharding sur cet index?\n",
    "2) Quels ingestion pipelines seraient pertinents pour notre sujet.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réponses :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1) Le sharding est un concept clé dans Elasticsearch qui consiste à diviser un index en plusieurs fragments appelés shards.   \n",
    "    Chaque shard est une unité autonome qui contient une partie des données de l'index.   \n",
    "    Le sharding permet de distribuer les données sur plusieurs nœuds d'un cluster Elasticsearch, ce qui permet d'obtenir une meilleure répartition de la charge   \n",
    "    et d'améliorer les performances de recherche et de récupération des données."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Dans le cas de notre index, nous pouvons imaginer un sharding basé sur un critère tel que l'identifiant unique de chaque document.  \n",
    "    Par exemple, nous pouvons choisir de sharder l'index en fonction d'une plage d'identifiants, de sorte que les documents ayant   \n",
    "    des identifiants similaires soient regroupés dans le même shard. Cela permettrait de répartir la charge de manière équilibrée et de faciliter les opérations de recherche et d'agrégation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2) En ce qui concerne les pipelines d'ingestion, plusieurs options pourraient être pertinentes pour notre sujet. Voici quelques exemples :\n",
    "\n",
    "        Le pipeline de normalisation : Ce pipeline pourrait inclure des étapes telles que la suppression des caractères spéciaux, la normalisation des mots en minuscules, la suppression des stopwords, etc. Cela permettrait de standardiser le texte avant l'indexation, ce qui faciliterait les opérations de recherche.\n",
    "\n",
    "        Le pipeline de détection des émotions : Ce pipeline pourrait utiliser des techniques de traitement du langage naturel pour détecter les émotions présentes dans le texte, par exemple en utilisant des modèles de classification pré-entrainés ou des lexiques émotionnels. Cela permettrait d'extraire automatiquement les informations sur les émotions des textes lors de l'indexation.\n",
    "\n",
    "        Le pipeline d'extraction des entités : Ce pipeline pourrait utiliser des techniques de reconnaissance d'entités nommées pour extraire des informations telles que les noms de personnes, les lieux, les organisations, etc. Cela permettrait d'enrichir les données avec des métadonnées supplémentaires, facilitant ainsi les requêtes spécifiques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"green\"> || - Alternatives :</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment aurait-on pu intégrer la gestion des stopwords au niveau du Mapping? (proposez un code)?\n",
    "\n",
    "Au niveau du Mapping, vous pouvez intégrer la gestion des stopwords en utilisant un \"analyzer\" personnalisé avec une liste de stopwords spécifiée. Voici un exemple de code pour intégrer la gestion des stopwords au niveau du Mapping dans Elasticsearch :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Se connecter à Elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "# Définir les stopwords personnalisés\n",
    "custom_stopwords = ['ive','something','feel','feeling','feelings','like','im','know','get','would','time','little','even','one','life','people','think','bit','things','much','dont','make','going']\n",
    "\n",
    "# Définir le mapping avec un analyseur personnalisé incluant les stopwords\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        \"content\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"custom_analyzer\"\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"custom_analyzer\": {\n",
    "                    \"type\": \"standard\",\n",
    "                    \"stopwords\": custom_stopwords\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Créer l'index avec le mapping personnalisé\n",
    "index_name = \"notes2\"\n",
    "es.indices.create(index=index_name, body={\"mappings\": mapping})\n",
    "\n",
    "# En appliquant ce mapping, le champ \"content\" sera analysé en utilisant l'analyseur \"analyseur_personnalise\", \n",
    "# qui supprimera les stopwords spécifiés lors de l'indexation et de la recherche."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment aurait-on pu intégrer le modèle de ML comme ingest pipeline? (proposez un code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
