{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"green\"> | - Questions théoriques :</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relire le cours sur Elastic Search à l’aide de nos nouvelles connaissance et répondre aux questions:\n",
    "1) Qu’est ce que le sharding, comment pourrait-on imaginer un sharding sur cet index?\n",
    "2) Quels ingestion pipelines seraient pertinents pour notre sujet.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réponses :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1) Le sharding est un concept clé dans Elasticsearch qui consiste à diviser un index en plusieurs fragments appelés shards.   \n",
    "    Chaque shard est une unité autonome qui contient une partie des données de l'index.   \n",
    "    Le sharding permet de distribuer les données sur plusieurs nœuds d'un cluster Elasticsearch, ce qui permet d'obtenir une meilleure répartition de la charge   \n",
    "    et d'améliorer les performances de recherche et de récupération des données."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Dans le cas de notre index, nous pouvons imaginer un sharding basé sur un critère tel que l'identifiant unique de chaque document.  \n",
    "    Par exemple, nous pouvons choisir de sharder l'index en fonction d'une plage d'identifiants, de sorte que les documents ayant   \n",
    "    des identifiants similaires soient regroupés dans le même shard. Cela permettrait de répartir la charge de manière équilibrée et de faciliter les opérations de recherche et d'agrégation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2) En ce qui concerne les pipelines d'ingestion, plusieurs options pourraient être pertinentes pour notre sujet. Voici quelques exemples :\n",
    "\n",
    "        Le pipeline de normalisation : Ce pipeline pourrait inclure des étapes telles que la suppression des caractères spéciaux, la normalisation des mots en minuscules, la suppression des stopwords, etc. Cela permettrait de standardiser le texte avant l'indexation, ce qui faciliterait les opérations de recherche.\n",
    "\n",
    "        Le pipeline de détection des émotions : Ce pipeline pourrait utiliser des techniques de traitement du langage naturel pour détecter les émotions présentes dans le texte, par exemple en utilisant des modèles de classification pré-entrainés ou des lexiques émotionnels. Cela permettrait d'extraire automatiquement les informations sur les émotions des textes lors de l'indexation.\n",
    "\n",
    "        Le pipeline d'extraction des entités : Ce pipeline pourrait utiliser des techniques de reconnaissance d'entités nommées pour extraire des informations telles que les noms de personnes, les lieux, les organisations, etc. Cela permettrait d'enrichir les données avec des métadonnées supplémentaires, facilitant ainsi les requêtes spécifiques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"green\"> || - Alternatives :</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment aurait-on pu intégrer la gestion des stopwords au niveau du Mapping? (proposez un code)?\n",
    "\n",
    "Au niveau du Mapping, vous pouvez intégrer la gestion des stopwords en utilisant un \"analyzer\" personnalisé avec une liste de stopwords spécifiée. Voici un exemple de code pour intégrer la gestion des stopwords au niveau du Mapping dans Elasticsearch :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'notes2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m# Créer l'index avec le mapping personnalisé\u001b[39;00m\n\u001b[1;32m     30\u001b[0m index_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnotes2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m es\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mcreate(index\u001b[39m=\u001b[39mnotes2, body\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mmappings\u001b[39m\u001b[39m\"\u001b[39m: mapping})\n\u001b[1;32m     33\u001b[0m \u001b[39m# En appliquant ce mapping, le champ \"content\" sera analysé en utilisant l'analyseur \"analyseur_personnalise\", \u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# qui supprimera les stopwords spécifiés lors de l'indexation et de la recherche.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'notes2' is not defined"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Se connecter à Elasticsearch\n",
    "es = Elasticsearch([{'host': 'localhost', 'port':9200, 'scheme':'http'}])\n",
    "\n",
    "# Définir les stopwords personnalisés\n",
    "custom_stopwords = ['ive','something','feel','feeling','feelings','like','im','know','get','would','time','little','even','one','life','people','think','bit','things','much','dont','make','going']\n",
    "\n",
    "# Définir le mapping avec un analyseur personnalisé incluant les stopwords\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        \"content\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"custom_analyzer\"\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"custom_analyzer\": {\n",
    "                    \"type\": \"standard\",\n",
    "                    \"stopwords\": custom_stopwords\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Créer l'index avec le mapping personnalisé\n",
    "index_name = \"notes2\"\n",
    "es.indices.create(index=notes2, body={\"mappings\": mapping})\n",
    "\n",
    "# En appliquant ce mapping, le champ \"content\" sera analysé en utilisant l'analyseur \"analyseur_personnalise\", \n",
    "# qui supprimera les stopwords spécifiés lors de l'indexation et de la recherche."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment aurait-on pu intégrer le modèle de ML comme ingest pipeline? (proposez un code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22263/1911423459.py:38: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  es.ingest.put_pipeline(id=pipeline_name, body={\"processors\": [ingest_processor]})\n"
     ]
    },
    {
     "ename": "SerializationError",
     "evalue": "Unable to serialize to JSON: {'processors': [{'set': {'field': 'emotion', 'value': {'script': {'source': '\\n                    def text = ctx.content;\\n                    return params.predict_function(text);\\n                ', 'lang': 'painless', 'params': {'predict_function': <function predict_classification at 0x7f2371acf790>}}}}}]} (type: dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/site-packages/elastic_transport/_serializer.py:108\u001b[0m, in \u001b[0;36mJsonSerializer.dumps\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjson_dumps(data)\n\u001b[1;32m    109\u001b[0m \u001b[39m# This should be captured by the .default()\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m# call but just in case we also wrap these.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/site-packages/elastic_transport/_serializer.py:79\u001b[0m, in \u001b[0;36mJsonSerializer.json_dumps\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjson_dumps\u001b[39m(\u001b[39mself\u001b[39m, data: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbytes\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39;49mdumps(\n\u001b[1;32m     80\u001b[0m         data, default\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault, ensure_ascii\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, separators\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m:\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     81\u001b[0m     )\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/json/__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n\u001b[0;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    235\u001b[0m     skipkeys\u001b[39m=\u001b[39;49mskipkeys, ensure_ascii\u001b[39m=\u001b[39;49mensure_ascii,\n\u001b[1;32m    236\u001b[0m     check_circular\u001b[39m=\u001b[39;49mcheck_circular, allow_nan\u001b[39m=\u001b[39;49mallow_nan, indent\u001b[39m=\u001b[39;49mindent,\n\u001b[1;32m    237\u001b[0m     separators\u001b[39m=\u001b[39;49mseparators, default\u001b[39m=\u001b[39;49mdefault, sort_keys\u001b[39m=\u001b[39;49msort_keys,\n\u001b[1;32m    238\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mencode(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/site-packages/elasticsearch/serializer.py:73\u001b[0m, in \u001b[0;36mJsonSerializer.default\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[0;32m---> 73\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to serialize \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m!r}\u001b[39;00m\u001b[39m (type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to serialize <function predict_classification at 0x7f2371acf790> (type: <class 'function'>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSerializationError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m# Create the ingest pipeline\u001b[39;00m\n\u001b[1;32m     37\u001b[0m pipeline_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mml_classification_pipeline\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 38\u001b[0m es\u001b[39m.\u001b[39;49mingest\u001b[39m.\u001b[39;49mput_pipeline(\u001b[39mid\u001b[39;49m\u001b[39m=\u001b[39;49mpipeline_name, body\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mprocessors\u001b[39;49m\u001b[39m\"\u001b[39;49m: [ingest_processor]})\n\u001b[1;32m     40\u001b[0m \u001b[39m# Test the ingest pipeline\u001b[39;00m\n\u001b[1;32m     41\u001b[0m document \u001b[39m=\u001b[39m {\n\u001b[1;32m     42\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mCeci est un exemple de texte à classifier\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/site-packages/elasticsearch/_sync/client/utils.py:414\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m \u001b[39mreturn\u001b[39;00m api(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/site-packages/elasticsearch/_sync/client/ingest.py:269\u001b[0m, in \u001b[0;36mIngestClient.put_pipeline\u001b[0;34m(self, id, description, error_trace, filter_path, human, if_version, master_timeout, meta, on_failure, pretty, processors, timeout, version)\u001b[0m\n\u001b[1;32m    267\u001b[0m     __body[\u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m version\n\u001b[1;32m    268\u001b[0m __headers \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39maccept\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent-type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m--> 269\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_request(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m    270\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPUT\u001b[39;49m\u001b[39m\"\u001b[39;49m, __path, params\u001b[39m=\u001b[39;49m__query, headers\u001b[39m=\u001b[39;49m__headers, body\u001b[39m=\u001b[39;49m__body\n\u001b[1;32m    271\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/site-packages/elasticsearch/_sync/client/_base.py:389\u001b[0m, in \u001b[0;36mNamespacedClient.perform_request\u001b[0;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mperform_request\u001b[39m(\n\u001b[1;32m    379\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    380\u001b[0m     method: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[39m# Use the internal clients .perform_request() implementation\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[39m# so we take advantage of their transport options.\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    390\u001b[0m         method, path, params\u001b[39m=\u001b[39;49mparams, headers\u001b[39m=\u001b[39;49mheaders, body\u001b[39m=\u001b[39;49mbody\n\u001b[1;32m    391\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/site-packages/elasticsearch/_sync/client/_base.py:285\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m     target \u001b[39m=\u001b[39m path\n\u001b[0;32m--> 285\u001b[0m meta, resp_body \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransport\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    286\u001b[0m     method,\n\u001b[1;32m    287\u001b[0m     target,\n\u001b[1;32m    288\u001b[0m     headers\u001b[39m=\u001b[39;49mrequest_headers,\n\u001b[1;32m    289\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    290\u001b[0m     request_timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_timeout,\n\u001b[1;32m    291\u001b[0m     max_retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_max_retries,\n\u001b[1;32m    292\u001b[0m     retry_on_status\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_on_status,\n\u001b[1;32m    293\u001b[0m     retry_on_timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_on_timeout,\n\u001b[1;32m    294\u001b[0m     client_meta\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client_meta,\n\u001b[1;32m    295\u001b[0m )\n\u001b[1;32m    297\u001b[0m \u001b[39m# HEAD with a 404 is returned as a normal response\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39m# since this is used as an 'exists' functionality.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m meta\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m \u001b[39m404\u001b[39m) \u001b[39mand\u001b[39;00m (\n\u001b[1;32m    300\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m meta\u001b[39m.\u001b[39mstatus \u001b[39m<\u001b[39m \u001b[39m299\u001b[39m\n\u001b[1;32m    301\u001b[0m     \u001b[39mand\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/site-packages/elastic_transport/_transport.py:307\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, target, body, headers, max_retries, retry_on_status, retry_on_timeout, request_timeout, client_meta)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcontent-type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m request_headers:\n\u001b[1;32m    304\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMust provide a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m'\u001b[39m\u001b[39m header to requests with bodies\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         )\n\u001b[0;32m--> 307\u001b[0m     request_body \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserializers\u001b[39m.\u001b[39;49mdumps(\n\u001b[1;32m    308\u001b[0m         body, mimetype\u001b[39m=\u001b[39;49mrequest_headers[\u001b[39m\"\u001b[39;49m\u001b[39mcontent-type\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     request_body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/site-packages/elastic_transport/_serializer.py:193\u001b[0m, in \u001b[0;36mSerializerCollection.dumps\u001b[0;34m(self, data, mimetype)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdumps\u001b[39m(\u001b[39mself\u001b[39m, data: Any, mimetype: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbytes\u001b[39m:\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_serializer(mimetype)\u001b[39m.\u001b[39;49mdumps(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.8/site-packages/elastic_transport/_serializer.py:112\u001b[0m, in \u001b[0;36mJsonSerializer.dumps\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m# This should be captured by the .default()\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m# call but just in case we also wrap these.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mUnicodeError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:  \u001b[39m# pragma: nocover\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mraise\u001b[39;00m SerializationError(\n\u001b[1;32m    113\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to serialize to JSON: \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m!r}\u001b[39;00m\u001b[39m (type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(data)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    114\u001b[0m         errors\u001b[39m=\u001b[39m(e,),\n\u001b[1;32m    115\u001b[0m     )\n",
      "\u001b[0;31mSerializationError\u001b[0m: Unable to serialize to JSON: {'processors': [{'set': {'field': 'emotion', 'value': {'script': {'source': '\\n                    def text = ctx.content;\\n                    return params.predict_function(text);\\n                ', 'lang': 'painless', 'params': {'predict_function': <function predict_classification at 0x7f2371acf790>}}}}}]} (type: dict)"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import texthero as hero\n",
    "from texthero import stopwords, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Load the trained model\n",
    "with open('nlp_pipeline.pkl', 'rb') as file:\n",
    "    nlp_pipeline = pickle.load(file)\n",
    "\n",
    "# Define the prediction function\n",
    "def predict_classification(text):\n",
    "    return nlp_pipeline.predict([text])[0]\n",
    "\n",
    "# Define the ingest processor\n",
    "ingest_processor = {\n",
    "    \"set\": {\n",
    "        \"field\": \"emotion\",\n",
    "        \"value\": {\n",
    "            \"script\": {\n",
    "                \"source\": \"\"\"\n",
    "                    def text = ctx.content;\n",
    "                    return params.predict_function(text);\n",
    "                \"\"\",\n",
    "                \"lang\": \"painless\",\n",
    "                \"params\": {\n",
    "                    \"predict_function\": predict_classification\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the ingest pipeline\n",
    "pipeline_name = \"ml_classification_pipeline\"\n",
    "es.ingest.put_pipeline(id=pipeline_name, body={\"processors\": [ingest_processor]})\n",
    "\n",
    "# Test the ingest pipeline\n",
    "document = {\n",
    "    \"content\": \"Ceci est un exemple de texte à classifier\"\n",
    "}\n",
    "result = es.index(index=\"notes2\", body=document, pipeline=pipeline_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
